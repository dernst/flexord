---
title: "Ordinal Clustering with the flex-Scheme"
subtitle: "Reproduction Code to the Simulation Study"
output: rmarkdown::html_vignette
bibliography: vignettes.bib
vignette: >
  %\VignetteIndexEntry{SimulationStudy}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
In this vignette, we provide reproduction code for the simulation study published
in *Ordinal Clustering with the flex-scheme* [@ernst_ordinal_2025]. An introduction
to the `flexord` package can be found [here](Intro2Flexord.html).

In the study, we have compared the performance of 12 different algorithms against
the *true clustermembership* via de Adjusted Rand Index [@hubert_arabie_1985]
on simulated ordinal data sets with equal response level lengths and no missing values.

We varied the following aspects of the data sets:

* the sample size $N \in \{50,200,500\}$;
* the number of response levels of each ordinal variable
  $r \in \{2,\ldots,11\}$;
* the number of variables $m \in \{3, 6, 11\}$; and
* the regularization parameter $\alpha \in \{0, 75, 150\}$, where
  more regularization results in more diffuse clusters.
  
For each configuration combination, we created `nIter=100` data sets, on
which we ran the algorithms.
  
We simplify this setup in this reproduction, by reducing `nIter` to 5, and
selecting only one configuration combination (for simplicity, we will choose
the medium difficulty option for all mentioned parameters). For actual reruns of
the simulation we recommend to re-increase `nIter` and the number of tested data
set characteristics.

```{r setup, message=F}
library(flexclust)
library(flexmix)
library(flexord) #data.table, cluster, nnet::multinom, possibly tidyverse stuff

library(parallel)
library(magrittr)
library(data.table)
library(nnet)
library(digest)

nIter <- 5
N <- 200
r <- 6 #possibly pick 7 to better distinguish from m
m <- 6
alpha <- 75
```
To simulate our `nIter` data sets, we

1) take the binary data on presence/absence of 11 symptoms of low backpain  in 464 patients and 
 their respective diagnoses provided by @fop_variable_2017 as input data,
2) on which we fit finite mixtures of multivariate independent Bernoulli distributions with
three^[The value of `k=3` is taken from the original study.] components (regularized by `alpha`,
in order to obtain moderately well separated clusters).
3) From these fitted models, we then generate data by drawing `N` times using binomial distributions
for components, where the number of trials is set to `r`.
4) Finally, we select the `m` variables of the simulated data set with the highest 'variable importance'.
We calculate this 'variable importance' by fitting a mixture of three multinomial regressions
explaining the expert diagnoses (and thus *true cluster memberships*) separately by each of the 11
original variables, and ordering them by decreasing Log-Likelihood.

Let us take a quick look at the input data set:

```{r inputdata}
data('lowbackpain')
lowbackpain$data |> str() #presence/absence of symptoms
lowbackpain$group |> str() #expert diagnoses
```

## Setup

To begin the simulation process, we create helper functions for simulating the data,
selecting the `m` variables with highest 'variable importance', and applying the parameters
mentioned above (plus error catching).

### Simulating the data from a `flexmix` model

```{r datasim}
data_sim_from_model <- function(mod, size,
                                N, alpha2=0) {
  prop <- prior(mod)
  param <- parameters(mod)
  k <- ncol(param)
  nvar <- nrow(param)
  ni <- round(N*prop)
  
  #true clusters
  z <- rep(seq(k), ni)
  
  #data
  x <- lapply(seq(k), \(i) {
    lapply(seq(nvar), \(var) {
      p <- param[var,i]
      rbinom(ni[i], size, p)
    }) %>% do.call(cbind, .)
  }) %>% do.call(rbind, .)
  
  shuffle <- sample(sum(ni), size=sum(ni),
                    replace=F)
  dat <- x[shuffle,]
  
  if(alpha2>0) {
    g <- sample(c(T,F), size=prod(dim(dat)),
                replace=T, prob=c(alpha2,(1-alpha2))) |> 
      matrix(nrow=nrow(dat))
    dat[g] <- sample(0:size, size=sum(g), replace=TRUE)
  }
  
  res <- list(group=z[shuffle], data=dat)
  res
}
```


### Variable selection by 'importance'
As mentioned, we select the `m` variables to be used in each of our `nboot` simulated data sets that show the lowest Log-Likelihood after fitting three-component multinomial regressions separately for each original variable:
```{r varimp}
stepGeneric <- function(fn, data, k, #simplify for the example
                        nrep = 10,
                        multicore = TRUE, clusterfun = clusters, 
                        max_by, verbose = FALSE,
                        set_seed = multicore) 
{
    progr = progress_eta(length(k) * nrep)
    step1 = function(ki, iter, seed) {
        if (verbose) {
            cat(sprintf("k=%d\t", ki))
            progr(iter)
        }
        if (set_seed) 
            set.seed(seed)
        tt = system.time({
            res = try(fn(ki))
        })
        if (!is(res, "try-error")) {
            newcls = clusterfun(res)
            data.table(k = ki, time = tt["elapsed"], nrep = nrep[1], 
                clusters = list(newcls), mod = list(res))
        }
        else {
            data.table(k = ki, time = tt["elapsed"], nrep = nrep[1], 
                clusters = list(NA), mod = list(NA), error = res)
        }
    }
    map = if (multicore) 
        mcmapply
    else mapply
    grid = expand.grid(k = k, rep = seq.int(nrep)) %>% as.data.table %>% 
        .[order(k, rep)] %>% .[, `:=`(iter, seq(.N))] %>% .[, 
        `:=`(seeds, .get_seeds(.N))]
    steps_each = map(function(k, iter, seed) {
        step1(k, iter, seed)
    }, grid$k, grid$iter, grid$seeds, SIMPLIFY = FALSE) %>% rbindlist(fill = TRUE)
    nrep1 = nrep
    steps_each[, `:=`(time, sum(time)), by = .(k)]
    steps_each[, `:=`(nrep, nrep1)]
    steps_each[, `:=`(n_success, sum(!is.na(mod))), by = .(k)]
    steps_each[, `:=`(crit, NA_real_)]
    steps_each[!is.na(mod), `:=`(crit, sapply(mod, max_by))]
    steps = steps_each[order(k, -crit)] %>% unique(by = "k") %>% 
        .[, -c("crit")]
    steps
}

sim_var_importance <- function(inputdata=lowbackpain$data,
                               inputgroup=lowbackpain$group) {
  
  mod1 <- stepGeneric(fn=\(k) flexmix(inputdata ~ 1, k=k,
                                      model=model, ...),
                      data=inputdata,
                      model=FLXMCbinomial(alpha2=0),
                      k=3, nrep=10,
                      verbose=F, multicore=T)
  sim_data <- data_sim_from_model(mod1$mod[[1]], 1, 500)
  
  cls <- as.factor(sim_data$group)
  
  mods <- lapply(seq(ncol(sim_data$data)), \(i) {
    sink('/dev/null')
    on.exit(sink(NULL))
    multinom(cls ~ sim_data$data[,i])
  })
  
  logliks <- sapply(mods, logLik)
  ord <- order(logliks, decreasing=TRUE)
  
  test <- sort(logliks, decreasing=TRUE) == logliks[order(logliks, decreasing=TRUE)]
    stopifnot(all(test))

    ord
}
```

## Applying the simulation process to the specified parameters
Another helper to parallelize the simulation process on the specified parameters, and add error catching:
```{r apply2params}
#is that necessary as a separate step?
sim_backpain_apply2params <- function(mod, #sim_backpain_sample_size_ncat
                                      stepfn,
                                      sizes,
                                      sample_size,
                                      n_vars,
                                      nIter,
                                      seed=0xBA0BAB,
                                      n_debug=NA,
                                      multicore=TRUE) {
  
  with_seed(seed, {
        sim1 <- \(size, N, n_vars, iter, seed, index) {
          set.seed(seed)
          progr(index)
          sim_data <- data_sim_from_model(mod, size, N)
          data <- sim_data$data
          group <- sim_data$group
          
          which_vars <- head(sim_var_importance(),
                             n_vars)
          data <- data[, which_vars]
          checksum <- digest::sha1(sim_data$data)
          
          m1 <- try(stepfn(data, k=3, size))
          
          if(!is(m1, "try-error")) {
            if(any(is.na(m1$clusters[[1]]))) {
                ari = NA
                } else {
                ari = randIndex(m1$clusters[[1]], group)
                }
            data.table(size, N, n_vars=n_vars,
                       iter, ari, checksum=checksum)
            } else {
              rr = \(debug=FALSE) {
                if(debug) debugonce(sim1)
                sim1(size, N, iter, seed, index)
                }
              data.table(size, N, n_vars=n_vars, iter,
                         error=m1, retry=rr, seed, checksum=checksum)
        }
    }

    if(is.data.table(mod)) {
        mod = mod$mod[[1]]
    }

    force(var_imp)

    grid = expand.grid(size=sizes, N=sample_size,
                       n_vars=n_vars, iter = seq(Niter))  |> 
        as.data.table() %>%
        .[, seed := .get_seeds(.N)] %>%
        .[sample(nrow(.), size=nrow(.), replace=FALSE),] %>%
        .[, index := seq(.N)]

#    grid = grid[N %in% c(50, 200, 500) & n_vars %in% c(3, 6, 11)]

    if(!is.na(n_debug)) {
        grid <- head(grid, n_debug)
    }

    # index neu wenn die Fortschrittsanzeige funktionieren soll
    grid[, index := seq(.N)]

    applyfn = if(multicore) mclapply else lapply

    progr <- progress_eta(nrow(grid))

    res <- applyfn(seq(nrow(grid)), \(i) {
        do.call(sim1, as.list(grid[i,]))
    }) |>  rbindlist(fill=TRUE)
    print(Sys.time())

    res
  })
}
```


## Running the simulation
Now we can finally apply the clustering algorithms to the simulated data sets:
```{r runit}
sim_backpain <- function(nIter, alpha2,
                         n_vars, sample_size,
                         size) { #leaving out @param algos for now
  
}

sim_backpain(nIter=nIter,
             alpha2=alpha,
             n_vars=m,
             sample_size=N,
             size=r)
```


## Visualization


#hi: when finished, remove unused bib entries from the bib file